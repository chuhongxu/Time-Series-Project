---
title: "ARMA/ARIMA/SARIMA Models"
format:
  html:
    code-fold: true
    number-sections: true
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(flipbookr)
library(tidyverse)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(reshape2)
library(tseries)
library(TTR)
```

# ARIMA Modeling

```{r, echo=FALSE}
walmart=read.csv("./data/Walmart_sales_data.csv")
type=read.csv("./data/Walmart_stores.csv")
df=merge(walmart,type,by="Store")
A_bytime=group_by(df[df$Type=='A',],Date)
A_sales=summarise(A_bytime,avg = mean(Weekly_Sales))
A_sales$Date=as.Date(A_sales$Date)
walmart$Date=as.Date(walmart$Date)
walmart_1.1=walmart[walmart$Store==1 & walmart$Dept==1,]
A_sales$isholiday=walmart_1.1$IsHoliday
temp.ts = ts(A_sales$avg, start=decimal_date(as.Date("2010-02-05")), frequency = 365.25/7)
# write.csv(A_sales, "A_sales.csv",row.names = FALSE)
```

In this part, I first use the weekly sales of type A stores from the Walmart dataset mentioned in Data Visualization and EDA tab.

## Check the stationarity

This is a visualization of this time series.

```{r,echo=FALSE}
ggplot(A_sales,aes(x=Date,y=avg))+
  geom_line()+
  labs(
    x = "Date",
    y = "Weekly sales"
  )+
  ggtitle("Weekly sales of type A stores")
```

Now review the ACF, PACF and Augmented Dickey-Fuller Test in EDA tab.

```{r}
ggAcf(temp.ts,150)+ggtitle("ACF graph of the weekly sales")
```

```{r}
ggPacf(temp.ts,150)+ggtitle("PACF graph of the weekly sales")
```

According to the ACF and PACF plots, this time series shows good stationarity besides a extreme value in ACF plot at the lag 52.

```{r, message=FALSE, warning=FALSE}
adf.test(temp.ts)
```

As for the ADF test, since p-value is less than 0.05, we have enough evidence to reject the null hypothesis and assert that this time series is stationary.

## Determine the parameter

Now use the ACF and PACF again to determine the parameter p and q in the ARIMA model.

```{r}
ggAcf(temp.ts,150)+ggtitle("ACF graph of the weekly sales")
```

From this ACF plot, we can observe that q is likely to be 1, 2, or 5.

```{r}
ggPacf(temp.ts,150)+ggtitle("PACF graph of the weekly sales")
```

From this PACF plot, we can predict that p is likely to be 1, 4, or 5.
Since we have not differenced the series, d is 0, and this is actually an ARMA model.

## Fit and choose the model

In this part, I will fit the ARIMA model with all possible parameters mentioned above, and use AIC, BIC and AICc as metrics to choose the best model. Here is the result.

```{r,warning=FALSE}
i=1
ls=matrix(rep(NA,6*9),nrow=9) 

for (p in c(1,2,5))
{
  for(q in c(1,4,5))
  {
    model<- Arima(temp.ts, order=c(p,0,q),include.drift = TRUE)
    ls[i,]= c(p,0,q,model$aic,model$bic,model$aicc)
    i=i+1
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

knitr::kable(temp)

```

```{r,echo=FALSE}
temp[which.min(temp$AIC),]
```

```{r,echo=FALSE}
#temp[which.min(temp$BIC),]
```

```{r,echo=FALSE}
#temp[which.min(temp$AICc),]
```

We can see that the model with parameters (p,d,q)=(1,0,5) has minimum AIC, BIC and AICc. Therefore, I choose this model as the best model.

Here is the summary of the ARIMA(1,0,5) model.

```{r}
fit <- Arima(temp.ts, order=c(1,0,5),include.drift = TRUE)
summary(fit)
```

**The equation of this ARIMA model:**

$$
X_t-0.7632X_{t-1}=\omega_t-0.4402\omega_{t-1}-0.1257\omega_{t-2}-0.2495\omega_{t-3}+0.3608\omega_{t-4}-0.5453\omega_{t-5}+19932.3528
$$

## Model diagnostic

```{r}
model_output <- capture.output(sarima(temp.ts, 1,0,5))
```

```{r}
checkresiduals(fit)
```

According to the Ljung-Box test, the residuals are independent, which means the model fits this time series well. However, from the Q-Q plot we can observe that a small fraction of the residuals do not follow the normal distribution.

## Auto.arima fitting

Try to use auto.arima function to fit an ARIMA model with this time series:

```{r}
auto.arima(temp.ts,seasonal = FALSE)
```

Auto.arima chooses parameters (p,d,q)=(2,0,2), which is different from what we have chosen in last part. According to the AIC and BIC, we notice that this model is not so good as the ARIMA(1,0,5) for our time series. The reason auto.arima function makes this choice is perhaps because seasonal component actually exists in the time series and it misleads the auto.arima when this function try to fit an ARIMA model. In fact, a SARIMA model might be better to fit this data than ARIMA model, and I will discuss this later.

## Forecasting

```{r}
fit %>% forecast(h=12) %>% autoplot()
```

This plot shows the forecasting of next three months and it seems to make sense.

```{r}
fit %>% forecast(h=52) %>% autoplot()
```

This plot shows the forecasting of next whole year. From this plot, we can clearly see that this ARIMA model actually does not fit the seasonal fluctuations.

## Benchmark comparison

In this part, I will compare the ARIMA model with several benchmark methods, including mean, naive, snaive and drift, using MAE and RMSE.

Use first 120 observations as training data and last 23 observations as test set. Then train the ARIMA model again on the training set.

```{r}
train = ts(A_sales$avg[1:120], start=decimal_date(as.Date("2010-02-05")), frequency = 365.25/7)
test=temp.ts[121:143]
train_fit=Arima(train, order=c(1,0,5),include.drift = TRUE) 
summary(train_fit)
```

```{r,warning=FALSE}
pred=forecast(train_fit,23)
res=pred$mean-test

pred_mean=meanf(train, h=23)
pred_naive=naive(train, h=23)
pred_snaive=snaive(train, h=23)
pred_drift=rwf(temp.ts, h=23, drift=TRUE)

res_mean=pred_mean$mean-test
res_naive=pred_naive$mean-test
res_snaive=pred_snaive$mean-test
res_drift=pred_drift$mean-test

mae_fit=mean(abs(res))
mae_mean=mean(abs(res_mean))
mae_naive=mean(abs(res_naive))
mae_snaive=mean(abs(res_snaive))
mae_drift=mean(abs(res_drift))

rmse_fit=sqrt(mean(res**2))
rmse_mean=sqrt(mean(res_mean**2))
rmse_naive=sqrt(mean(res_naive**2))
rmse_snaive=sqrt(mean(res_snaive**2))
rmse_drift=sqrt(mean(res_drift**2))

temp=data.frame(
  methods=c("ARIMA", "mean", "naive", "snaive", "drift"),
  MAE=c(mae_fit, mae_mean, mae_naive, mae_snaive, mae_drift),
  RMSE=c(rmse_fit, rmse_mean, rmse_naive, rmse_snaive, rmse_drift)
)
knitr::kable(temp)
```

From this result, we can find that the ARIMA model trained on the training set does not perform better than some of the benchmark methods.

Following is a graph that displays the forecasting of ARIMA and benchmark methods for next half year.

```{r,warning=FALSE}
autoplot(temp.ts) +
  autolayer(meanf(temp.ts, h=26),
            series="Mean", PI=FALSE) +
  autolayer(naive(temp.ts, h=26),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(temp.ts, h=26),
            series="SNaïve", PI=FALSE)+
  autolayer(rwf(temp.ts, h=26, drift=TRUE),
            series="Drift", PI=FALSE)+
  autolayer(forecast(fit,26), 
            series="fit",PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
```

There might be several reasons that this ARIMA model do not perform better than benchmark methods. First, the training set is relatively small, which only contains data almost in two years. Second, there exists seasonal component every year, but a basic ARIMA model can not fit the underline seasonal pattern well. In further analysis, a SARIMA can be used to enhance the performance.

# SARIMA Modeling

Now, let's go back to review this ACF plot again.

```{r}
ggAcf(temp.ts,150)+ggtitle("ACF graph of the weekly sales")
```

In this plot, it is clear that a significant value appears at lag 52, which is actually the number of weeks in a year. This phenomenon suggests strong seasonal effects in this time series. Therefore, it is appropriate to fit a SARIMA model, which takes seasonal components into consideration, compared to a traditional ARIMA model that we have fitted in the above section.

Additionally, in order to more comprehensively analyze the sales of Walmart, I will also respectively consider the weekly sales of type B and C stores later.

## Differencing and seasonal differencing

Apply first order differencing to the time series and plot:

```{r}
temp.ts %>% diff() %>% ggtsdisplay()

```

The ACF and PACF show relatively good stationarity.

Now try to apply a seasonal differencing where the seasonal lag is 52.

```{r}
temp.ts %>% diff(lag=52) %>% ggtsdisplay()
```

The ACF and PACF indicate much better stationarity compared to first order differencing.
Thus, for the parameters of SARIMA model, I will consider:\
d=0,1\
D=1\
p=0,1\
P=1,2\
q=0,1\
Q=1,2

## Fit and choose the model

```{r,warning=FALSE}
i=1
ls=matrix(rep(NA,9*12),nrow=12) 

for (p in 0:1)
{
  for(q in 0:1)
  {
    for(P in 1:2)
    {
      for(Q in 1:2)
        {
        for(d in 0:1){
          model<- try(Arima(temp.ts,order=c(p,d,q),seasonal=c(P,1,Q)),silent = TRUE)
          if('try-error' %in% class(model)){
            next
            }else{
            ls[i,]= c(p,d,q,P,1,Q,model$aic,model$bic,model$aicc)
            i=i+1
          }
        }
      }
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")

#temp
knitr::kable(temp)
```

```{r,echo=FALSE}
temp[which.min(temp$AIC),]
```

```{r,echo=FALSE}
# temp[which.min(temp$BIC),]
```

```{r,echo=FALSE}
# temp[which.min(temp$AICc),]
```

We can see that the model with parameters (p,d,q,P,D,Q)=(0,1,1,1,1,1) has minimum AIC, BIC and AICc. Therefore, I choose this model as the best model.

Here is the summary of the SARIMA(0,1,1,1,1,1,52) model.

```{r}
fit=Arima(temp.ts,order=c(0,1,1),seasonal=c(1,1,1))
summary(fit)
```

Model diagnostic:

```{r}
checkresiduals(fit)
```

According to the Ljung-Box test, the residuals are independent, which means the SARIMA model fits this time series well.

## Auto.arima fitting

Try to use auto.arima function to fit a SARIMA model with this time series:

```{r,warning=FALSE}
auto.arima(temp.ts,seasonal = TRUE)
```

auto.arima function fits a SARIMA(0,1,2,0,1,0)[52] model, which is different from the SARIMA(0,1,1,1,1,1)[52] that we chose just now. The AIC and AICc of this model is larger than our chosen model, while BIC of this model is slightly smaller due to less parameters. Actually, both models have relatively good performance and they have very close AIC, BIC and AICc values. The results of auto.arima this time make much more sense than in ARIMA section early in this page.

## Forecasting

```{r}
fit %>% forecast(h=52) %>% autoplot()
```

This is the forecasting for the next whole year with our SARIMA model. It is obvious that this model has fitted the seasonal effect quite well.

## Benchmark comparison

Similar to ARIMA section, I will compare the SARIMA model with several benchmark methods, including mean, naive, snaive and drift, using MAE and RMSE.

Use first 120 observations as training data and last 23 observations as test set. Then train the SARIMA model again on the training set.

```{r,warning=FALSE}
train = ts(A_sales$avg[1:120], start=decimal_date(as.Date("2010-02-05")), frequency = 365.25/7)
test=temp.ts[121:143]
train_fit=Arima(train,order=c(0,1,1),seasonal=c(1,1,1))
summary(train_fit)

```

```{r,warning=FALSE}
pred=forecast(train_fit,23)
res=pred$mean-test

pred_mean=meanf(train, h=23)
pred_naive=naive(train, h=23)
pred_snaive=snaive(train, h=23)
pred_drift=rwf(temp.ts, h=23, drift=TRUE)

res_mean=pred_mean$mean-test
res_naive=pred_naive$mean-test
res_snaive=pred_snaive$mean-test
res_drift=pred_drift$mean-test

mae_fit=mean(abs(res))
mae_mean=mean(abs(res_mean))
mae_naive=mean(abs(res_naive))
mae_snaive=mean(abs(res_snaive))
mae_drift=mean(abs(res_drift))

rmse_fit=sqrt(mean(res**2))
rmse_mean=sqrt(mean(res_mean**2))
rmse_naive=sqrt(mean(res_naive**2))
rmse_snaive=sqrt(mean(res_snaive**2))
rmse_drift=sqrt(mean(res_drift**2))

temp=data.frame(
  methods=c("SARIMA", "mean", "naive", "snaive", "drift"),
  MAE=c(mae_fit, mae_mean, mae_naive, mae_snaive, mae_drift),
  RMSE=c(rmse_fit, rmse_mean, rmse_naive, rmse_snaive, rmse_drift)
)
knitr::kable(temp)
```

According to this result table, SARIMA outperforms all the benchmark methods without any doubt. Both MAE and RMSE of the model are much less than benchmarks. This means the SARIMA model performs well and it can fit the pattern within the data.

Following is a graph that displays the forecasting of ARIMA and benchmark methods for next half year.

```{r,warning=FALSE}
autoplot(temp.ts) +
  autolayer(meanf(temp.ts, h=26),
            series="Mean", PI=FALSE) +
  autolayer(naive(temp.ts, h=26),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(temp.ts, h=26),
            series="SNaïve", PI=FALSE)+
  autolayer(rwf(temp.ts, h=26, drift=TRUE),
            series="Drift", PI=FALSE)+
  autolayer(forecast(fit,26), 
            series="fit",PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
```

## Cross-validation

In this part, I will try seasonal cross validation using 1 step ahead forecasts and 52 steps (the seasonal period of this data) ahead forecasts and compare the RMSE of our chosen SARIMA model and the auto.arima model.

```{r}
set.seed(1001)

farima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1),seasonal = c(1,1,1)), h=h)}
farima2 <- function(x, h){forecast(Arima(x, order=c(0,1,2),seasonal = c(0,1,0)), h=h)}

e1 <- tsCV(temp.ts, farima1, h=1)
e2 <- tsCV(temp.ts, farima2, h=1)
RMSE1=sqrt(mean(e1^2, na.rm=TRUE))
RMSE2=sqrt(mean(e2^2, na.rm=TRUE))

e3 <- tsCV(temp.ts, farima1, h=52)
e4 <- tsCV(temp.ts, farima2, h=52)
RMSE3=sqrt(mean(e3^2, na.rm=TRUE))
RMSE4=sqrt(mean(e4^2, na.rm=TRUE))
```


```{r}
temp=data.frame(
  Models=c("Chosen model", "Auto model"),
  step_1=c(RMSE1, RMSE2),
  step_52=c(RMSE3, RMSE4)
)
knitr::kable(temp)
```

This RMSE result table shows that our chosen model SARIMA(0,1,1,1,1,1)[52] have much smaller RMSE for both 1 step and 52 steps cross-validation, which means our model performs better than the model selected by auto.arima.

## Type B and C stores

In this part, I will fit SARIMA models for the weekly sales of type B and C stores. First, let's have a quick look of these two time series.

```{r, echo=FALSE}
B_bytime=group_by(df[df$Type=='B',],Date)
B_sales=summarise(B_bytime,avg = mean(Weekly_Sales))
B_sales$Date=as.Date(B_sales$Date)
B_sales$isholiday=walmart_1.1$IsHoliday
temp.ts.b = ts(B_sales$avg, start=decimal_date(as.Date("2010-02-05")), frequency = 365.25/7)

C_bytime=group_by(df[df$Type=='C',],Date)
C_sales=summarise(C_bytime,avg = mean(Weekly_Sales))
C_sales$Date=as.Date(C_sales$Date)
C_sales$isholiday=walmart_1.1$IsHoliday
temp.ts.c = ts(C_sales$avg, start=decimal_date(as.Date("2010-02-05")), frequency = 365.25/7)
```

```{r,echo=FALSE}
ggplot(B_sales,aes(x=Date,y=avg))+
  geom_line()+
  labs(
    x = "Date",
    y = "Weekly sales"
  )+
  ggtitle("Weekly sales of type B stores")
```

```{r,echo=FALSE}
ggplot(C_sales,aes(x=Date,y=avg))+
  geom_line()+
  labs(
    x = "Date",
    y = "Weekly sales"
  )+
  ggtitle("Weekly sales of type C stores")
```

We can notice that the data pattern of type B stores are very similar to type A stores, but the weekly sales of type C stores look quite different.

Now, fit different SARIMA models for type B and C stores then choose the best models based on AIC, BIC and AICc.

```{r}
i=1
ls=matrix(rep(NA,9*32),nrow=32) 

for (p in 0:1)
{
  for(q in 0:1)
  {
    for(P in 0:1)
    {
      for(Q in 0:1)
        {
        for(d in 0:1){
          model<- try(Arima(temp.ts.b,order=c(p,d,q),seasonal=c(P,1,Q)),silent = TRUE)
          if('try-error' %in% class(model)){
            next
            }else{
            ls[i,]= c(p,d,q,P,1,Q,model$aic,model$bic,model$aicc)
            i=i+1
          }
        }
      }
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")

```

```{r,echo=FALSE}
temp[which.min(temp$AIC),]
```

For type B stores, the best model with minimum AIC, BIC and AICc is SARIMA(0,1,1,1,1,0)[52]. The parameters of this model is almost the same as those we discussed for type A stores except Q.

```{r}
fit=Arima(temp.ts.b,order=c(0,1,1),seasonal=c(1,1,0))
fit %>% forecast(h=52) %>% autoplot()
```

This forecasting above shows that the model captured the seasonal effect well.

```{r}
i=1
ls=matrix(rep(NA,9*32),nrow=32) 

for (p in 0:1)
{
  for(q in 0:1)
  {
    for(P in 0:1)
    {
      for(Q in 0:1)
        {
        for(d in 0:1){
          model<- try(Arima(temp.ts.c,order=c(p,d,q),seasonal=c(P,1,Q)),silent = TRUE)
          if('try-error' %in% class(model)){
            next
            }else{
            ls[i,]= c(p,d,q,P,1,Q,model$aic,model$bic,model$aicc)
            i=i+1
          }
        }
      }
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","P","D","Q","AIC","BIC","AICc")

```

```{r,echo=FALSE}
temp[which.min(temp$AIC),]
```

As for type C stores, the number of model parameters are exactly the same as type B stores. The optimal model is also SARIMA(0,1,1,1,1,0)[52].

```{r}
fit=Arima(temp.ts.c,order=c(0,1,1),seasonal=c(1,1,0))
fit %>% forecast(h=52) %>% autoplot()
```

In a conclusion, although type A, B and C stores have different sizes, the SARIMA models fitted with the data have similar parameters and same seasonal period. Additionally, the data patterns of weekly sales for type B and C stores look very different, but the final models of these two types of store selected based on AIC, BIC and AICc have exactly the same numbers of parameters, which suggests that the hidden patterns and seasonal components are actually similar.
